{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f529a8a",
   "metadata": {},
   "source": [
    "# CGNet: A Convolutional Neural Network for Vegetation Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acbc69",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b01bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, sys, os\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from cgnet import CGNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score, f1_score\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from image_preprocessing.image_preprocessing import load_image_pairs_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec572f",
   "metadata": {},
   "source": [
    "### Prepare Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca868de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VegetationChangeDataset(Dataset):\n",
    "    def __init__(self, image_pairs, labels, transform=None):\n",
    "        \"\"\"\n",
    "        image_pairs: list of tuples (img1, img2) - each img shape [H, W, 3]\n",
    "        labels: list of binary vegetation change maps - each label shape [H, W]\n",
    "        transform: transform both images to tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.image_pairs = image_pairs\n",
    "        self.labels = labels\n",
    "        self.transform = transform or T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1, img2 = self.image_pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply transforms (ToTensor converts HWC to CHW)\n",
    "        x1 = self.transform(img1)\n",
    "        x2 = self.transform(img2)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.from_numpy(label).float().unsqueeze(0)  # [1, H, W]\n",
    "\n",
    "        return x1, x2, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f828a",
   "metadata": {},
   "source": [
    "### Define Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e80bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonaf\\anaconda3\\envs\\CGNet\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jonaf\\anaconda3\\envs\\CGNet\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = CGNet(pretrained=True)\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use logits for numerical stability\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6f65d",
   "metadata": {},
   "source": [
    "### Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d49d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../Data/Antwerpen/Antwerpen_2018/JPEG2000/OMWRGB18VL_11002.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Antwerpen/Antwerpen_2022/JPEG2000/OMWRGB22VL_11002.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Leuven/Leuven_2018/JPEG2000/OMWRGB18VL_24062.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Leuven/Leuven_2022/JPEG2000/OMWRGB22VL_24062.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Kortrijk/Kortrijk_2018/JPEG2000/OMWRGB18VL_34022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Kortrijk/Kortrijk_2022/JPEG2000/OMWRGB22VL_34022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Brugge/Brugge_2018/JPEG2000/OMWRGB18VL_31005.jp2 into shape (3, 8000, 6500)\n",
      "Reading ../../Data/Brugge/Brugge_2022/JPEG2000/OMWRGB22VL_31005.jp2 into shape (3, 8000, 6500)\n",
      "Reading ../../Data/Hasselt/Hasselt_2018/JPEG2000/OMWRGB18VL_71022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Hasselt/Hasselt_2022/JPEG2000/OMWRGB22VL_71022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Mechelen/Mechelen_2018/JPEG2000/OMWRGB18VL_12025.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Mechelen/Mechelen_2022/JPEG2000/OMWRGB22VL_12025.jp2 into shape (3, 8500, 7000)\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\n",
    "    ('../../Data/Antwerpen/Antwerpen_2018/JPEG2000/OMWRGB18VL_11002.jp2', '../../Data/Antwerpen/Antwerpen_2022/JPEG2000/OMWRGB22VL_11002.jp2', 8500, 7000, 4420, 6980, 3320, 5880, 256),\n",
    "    ('../../Data/Leuven/Leuven_2018/JPEG2000/OMWRGB18VL_24062.jp2', '../../Data/Leuven/Leuven_2022/JPEG2000/OMWRGB22VL_24062.jp2', 8500, 7000, 3620, 6180, 2320, 4880, 256),\n",
    "    ('../../Data/Kortrijk/Kortrijk_2018/JPEG2000/OMWRGB18VL_34022.jp2', '../../Data/Kortrijk/Kortrijk_2022/JPEG2000/OMWRGB22VL_34022.jp2', 8500, 7000, 2120, 4680, 1520, 4080, 256),\n",
    "    ('../../Data/Brugge/Brugge_2018/JPEG2000/OMWRGB18VL_31005.jp2', '../../Data/Brugge/Brugge_2022/JPEG2000/OMWRGB22VL_31005.jp2', 8000, 6500, 4470, 7030, 2020, 4580, 256),\n",
    "    ('../../Data/Hasselt/Hasselt_2018/JPEG2000/OMWRGB18VL_71022.jp2', '../../Data/Hasselt/Hasselt_2022/JPEG2000/OMWRGB22VL_71022.jp2', 8500, 7000, 2570, 5130, 3020, 5580, 256),\n",
    "    ('../../Data/Mechelen/Mechelen_2018/JPEG2000/OMWRGB18VL_12025.jp2', '../../Data/Mechelen/Mechelen_2022/JPEG2000/OMWRGB22VL_12025.jp2', 8500, 7000, 3570, 6130, 3020, 5580, 256),\n",
    "               ]\n",
    "\n",
    "image_pairs_train, labels_train = load_image_pairs_labels(image_paths, normalized=False)\n",
    "\n",
    "# cleanup\n",
    "if 255 in np.unique(labels_train):\n",
    "   labels_train = np.clip(labels_train, 0, 1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41733d03",
   "metadata": {},
   "source": [
    "### Creating Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bc1a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../Data/Gent/Gent_2020/JPEG2000/OMWRGB20VL_44021.jp2 into shape (3, 8500, 7000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../Data/Gent/Gent_2024/JPEG2000/OMWRGB24VL_44021.jp2 into shape (3, 8500, 7000)\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\n",
    "    ('../../Data/Gent/Gent_2020/JPEG2000/OMWRGB20VL_44021.jp2', '../../Data/Gent/Gent_2024/JPEG2000/OMWRGB24VL_44021.jp2', 8500, 7000, 4220, 6780, 2520, 5080, 256)\n",
    "               ]\n",
    "\n",
    "test_image_pairs, test_labels = load_image_pairs_labels(image_paths, normalized=False)\n",
    "\n",
    "# cleanup\n",
    "if 255 in np.unique(test_labels):\n",
    "   test_labels = np.clip(test_labels, 0, 1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbefb34",
   "metadata": {},
   "source": [
    "### Fine-Tuning Loop (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e846fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5220\n",
      "Epoch 2, Loss: 0.4510\n",
      "Epoch 3, Loss: 0.4250\n",
      "Epoch 4, Loss: 0.4021\n",
      "Epoch 5, Loss: 0.3738\n",
      "Epoch 6, Loss: 0.3547\n",
      "Epoch 7, Loss: 0.3372\n",
      "Epoch 8, Loss: 0.3322\n",
      "Epoch 9, Loss: 0.3157\n",
      "Epoch 10, Loss: 0.3050\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "train_dataset = VegetationChangeDataset(image_pairs_train, labels_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(10):  # You can increase this\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x1, x2, label in train_loader:\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        change_map, _ = model(x1, x2)  # Use only the first output\n",
    "        loss = criterion(change_map, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637dce35",
   "metadata": {},
   "source": [
    "### Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61fe63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, threshold=0.5, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in dataloader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds, _ = model(x1, x2)  # [B, 1, H, W]\n",
    "            preds = torch.sigmoid(preds).squeeze(1)  # [B, H, W]\n",
    "            labels = labels.squeeze(1)\n",
    "\n",
    "            # Binarize predictions\n",
    "            preds_bin = (preds > threshold).int().cpu().numpy()\n",
    "            labels_bin = labels.int().cpu().numpy()\n",
    "\n",
    "            # Flatten and collect\n",
    "            for p, l in zip(preds_bin, labels_bin):\n",
    "                all_preds.append(p.flatten())\n",
    "                all_labels.append(l.flatten())\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    iou = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"IoU\": iou,\n",
    "        \"f1\": f1,\n",
    "    }, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8df536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7736\n",
      "Precision: 0.5432\n",
      "Recall: 0.1547\n",
      "IoU: 0.1369\n",
      "f1: 0.2408\n"
     ]
    }
   ],
   "source": [
    "# Prepare test set\n",
    "test_dataset = VegetationChangeDataset(test_image_pairs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate\n",
    "metrics, y_pred, y_true = evaluate_model(model, test_loader, device=device)\n",
    "\n",
    "# Show results\n",
    "for name, value in metrics.items():\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d147f2b",
   "metadata": {},
   "source": [
    "### Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a024e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cgnet_vegetation_finetuned.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CGNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
