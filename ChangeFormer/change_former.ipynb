{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0959ac",
   "metadata": {},
   "source": [
    "# A Transformer-Based Siamese Network for vegetation Change Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be75d9d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5549dccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonaf\\anaconda3\\envs\\CGNet\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jonaf\\anaconda3\\envs\\CGNet\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, jaccard_score\n",
    "from models.ChangeFormer import ChangeFormerV3\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from image_preprocessing.image_preprocessing import load_image_pairs_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9add9",
   "metadata": {},
   "source": [
    "### Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396eeaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../Data/Antwerpen/Antwerpen_2018/JPEG2000/OMWRGB18VL_11002.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Antwerpen/Antwerpen_2022/JPEG2000/OMWRGB22VL_11002.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Leuven/Leuven_2018/JPEG2000/OMWRGB18VL_24062.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Leuven/Leuven_2022/JPEG2000/OMWRGB22VL_24062.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Kortrijk/Kortrijk_2018/JPEG2000/OMWRGB18VL_34022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Kortrijk/Kortrijk_2022/JPEG2000/OMWRGB22VL_34022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Brugge/Brugge_2018/JPEG2000/OMWRGB18VL_31005.jp2 into shape (3, 8000, 6500)\n",
      "Reading ../../Data/Brugge/Brugge_2022/JPEG2000/OMWRGB22VL_31005.jp2 into shape (3, 8000, 6500)\n",
      "Reading ../../Data/Hasselt/Hasselt_2018/JPEG2000/OMWRGB18VL_71022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Hasselt/Hasselt_2022/JPEG2000/OMWRGB22VL_71022.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Mechelen/Mechelen_2018/JPEG2000/OMWRGB18VL_12025.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Mechelen/Mechelen_2022/JPEG2000/OMWRGB22VL_12025.jp2 into shape (3, 8500, 7000)\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\n",
    "    ('../../Data/Antwerpen/Antwerpen_2018/JPEG2000/OMWRGB18VL_11002.jp2', '../../Data/Antwerpen/Antwerpen_2022/JPEG2000/OMWRGB22VL_11002.jp2', 8500, 7000, 4420, 6980, 3320, 5880, 256),\n",
    "    ('../../Data/Leuven/Leuven_2018/JPEG2000/OMWRGB18VL_24062.jp2', '../../Data/Leuven/Leuven_2022/JPEG2000/OMWRGB22VL_24062.jp2', 8500, 7000, 3620, 6180, 2320, 4880, 256),\n",
    "    ('../../Data/Kortrijk/Kortrijk_2018/JPEG2000/OMWRGB18VL_34022.jp2', '../../Data/Kortrijk/Kortrijk_2022/JPEG2000/OMWRGB22VL_34022.jp2', 8500, 7000, 2120, 4680, 1520, 4080, 256),\n",
    "    ('../../Data/Brugge/Brugge_2018/JPEG2000/OMWRGB18VL_31005.jp2', '../../Data/Brugge/Brugge_2022/JPEG2000/OMWRGB22VL_31005.jp2', 8000, 6500, 4470, 7030, 2020, 4580, 256),\n",
    "    ('../../Data/Hasselt/Hasselt_2018/JPEG2000/OMWRGB18VL_71022.jp2', '../../Data/Hasselt/Hasselt_2022/JPEG2000/OMWRGB22VL_71022.jp2', 8500, 7000, 2570, 5130, 3020, 5580, 256),\n",
    "    ('../../Data/Mechelen/Mechelen_2018/JPEG2000/OMWRGB18VL_12025.jp2', '../../Data/Mechelen/Mechelen_2022/JPEG2000/OMWRGB22VL_12025.jp2', 8500, 7000, 3570, 6130, 3020, 5580, 256),\n",
    "               ]\n",
    "\n",
    "image_pairs_train, labels_train = load_image_pairs_labels(image_paths, normalized=False)\n",
    "\n",
    "# cleanup\n",
    "if 255 in np.unique(labels_train):\n",
    "   labels_train = np.clip(labels_train, 0, 1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d496b54",
   "metadata": {},
   "source": [
    "### Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83873947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../Data/Gent/Gent_2020/JPEG2000/OMWRGB20VL_44021.jp2 into shape (3, 8500, 7000)\n",
      "Reading ../../Data/Gent/Gent_2024/JPEG2000/OMWRGB24VL_44021.jp2 into shape (3, 8500, 7000)\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\n",
    "    ('../../Data/Gent/Gent_2020/JPEG2000/OMWRGB20VL_44021.jp2', '../../Data/Gent/Gent_2024/JPEG2000/OMWRGB24VL_44021.jp2', 8500, 7000, 4220, 6780, 2520, 5080, 256)\n",
    "               ]\n",
    "\n",
    "test_image_pairs, test_labels = load_image_pairs_labels(image_paths, normalized=False)\n",
    "\n",
    "# cleanup\n",
    "if 255 in np.unique(test_labels):\n",
    "   test_labels = np.clip(test_labels, 0, 1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c4c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeDataset(Dataset):\n",
    "    def __init__(self, image_pairs, labels):\n",
    "        self.image_pairs = image_pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1, img2 = self.image_pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Convert to tensors\n",
    "        img1 = torch.from_numpy(img1.transpose(2, 0, 1)).float()  # [C, H, W]\n",
    "        img2 = torch.from_numpy(img2.transpose(2, 0, 1)).float()\n",
    "        label = torch.from_numpy(label).long()  # [H, W]\n",
    "        return img1, img2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73a02964",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = ChangeDataset(image_pairs_train, labels_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = ChangeDataset(test_image_pairs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51201072",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ChangeFormerV3(input_nc=3, output_nc=2, decoder_softmax=False).to(device)\n",
    "\n",
    "# ========= Loss & Optimizer ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd84e4",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ffb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5584\n",
      "Epoch [2/10], Loss: 0.5071\n",
      "Epoch [3/10], Loss: 0.5119\n",
      "Epoch [4/10], Loss: 0.5032\n",
      "Epoch [5/10], Loss: 0.4993\n",
      "Epoch [6/10], Loss: 0.4994\n",
      "Epoch [7/10], Loss: 0.4944\n",
      "Epoch [8/10], Loss: 0.4863\n",
      "Epoch [9/10], Loss: 0.4820\n",
      "Epoch [10/10], Loss: 0.4549\n"
     ]
    }
   ],
   "source": [
    "def train(num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for img1, img2, label in train_loader:\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img1, img2)  # output: [B, 2, H, W]\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce19c2",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34426bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(img1, img2, gt, pred, idx):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "    axs[0].imshow(img1.transpose(1, 2, 0).astype(np.uint8))\n",
    "    axs[0].set_title(\"Image T1\")\n",
    "    axs[1].imshow(img2.transpose(1, 2, 0).astype(np.uint8))\n",
    "    axs[1].set_title(\"Image T2\")\n",
    "    axs[2].imshow(gt, cmap='gray')\n",
    "    axs[2].set_title(\"Ground Truth\")\n",
    "    axs[3].imshow(pred, cmap='gray')\n",
    "    axs[3].set_title(\"Prediction\")\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"prediction_vs_gt_{idx}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b682ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7628\n",
      "Precision: 0.4534\n",
      "Recall: 0.1077\n",
      "IoU: 0.0953\n",
      "f1: 0.1740\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (img1, img2, label) in enumerate(test_loader):\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            output = model(img1, img2)  # [1, 2, H, W]\n",
    "            pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "            y_true.extend(label.numpy().flatten())\n",
    "            y_pred.extend(pred.flatten())\n",
    "\n",
    "            # Visualization (first 5 only)\n",
    "            # if idx < 5:\n",
    "            #     visualize_predictions(\n",
    "            #         img1.cpu().numpy()[0],\n",
    "            #         img2.cpu().numpy()[0],\n",
    "            #         label.numpy()[0],\n",
    "            #         pred,\n",
    "            #         idx\n",
    "            #     )\n",
    "\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    iou = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"IoU\": iou,\n",
    "        \"f1\": f1,\n",
    "    }, y_pred, y_true\n",
    "\n",
    "metrics, y_pred, y_true = evaluate()\n",
    "\n",
    "for name, value in metrics.items():\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d872f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437402a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"change_former.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CGNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
